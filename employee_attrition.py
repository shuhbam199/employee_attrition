# -*- coding: utf-8 -*-
"""Employee_attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11R82ZQWlaSD4YXmYyU7kWLkgKm64diya

<a href="https://colab.research.google.com/github/shuhbam199/FebGithub/blob/main/Employee_attrition.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io
!gdown 16KtxSt_QEGQvfluEaMls5cCHPwhRXgCk

df = pd.read_csv("HR-Employee-Attrition.csv")
df.info()

df['Attrition'].value_counts()

df_new =  df[['MaritalStatus_Single','EducationField','JobRole','MaritalStatus_Married','StockOptionLevel','JobSatisfaction','MonthlyIncome','MonthlyRate','OverTime','Age']]

"""No field with null cells"""

df.iloc[:, 15:]

df.hist(figsize = (20,20))
plt.show()

"""What can we observe from these plots ?


Many histograms are tail-heavy

Lot of attributes are right-skewed
(e.g. MonthlyIncome DistanceFromHome, YearsAtCompany)

Data transformation methods may be required for standardisation

Recall why standardisation is preferred ?
Some features seem to have normal distributions

Eg: Age:
Slightly right-skewed normal distribution
Bulk of the staff between 25 and 45 years old
Some features are constant

Eg: EmployeeCount and StandardHours are constant values for all employees.

They're likely to be redundant features.

How can these features contribute to our problem ?
Constant features are not in any way useful for predictions
So we can drop these features from the dataset
Some features seem to be uniformly distributed.

Eg: EmployeeNumber

Uniformly distributed and constant features won't contribute to our analysis. Why?

Each value is equally likely to occur
So what should we do ?
We can drop these features from our dataset
Some features are categorical i.e binomially/multinomially distributed

Eg: WorkLifeBalance, StockOptionLevel etc

Can we use these features directly in our problem ?
No. They willl first have to be encoded
Recall which encoding has to be used for which features
Binary Encoding (0/1) : Features with only 2 unique values

Label Encoding (0, 1, 2, 3 ....) : More than 2 unique values having a particular order

OneHot Encoding ([0 0 0 1], ...) : More than 2 unique values having no order

Target encoding ([0.1, 0.33, .....)] : Features with a lot of unique vals having no order

We can also see from these features that their ranges vary a lot

Recall why different feature scales can be a problem

We will deal with this problem later

First, lets remove the features that won't contribute to our analysis
"""

df.drop(['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], axis=1, inplace=True)

"""Now lets encode our categorical features

#### Which encoding technique should we use ?

  - It depends upon:
    - Number of unique values a feature has
    - If there is a sequence between the feature vals

Lets first check how many unique values each feature has

"""

def unique_vals(col):

  if col.dtype == "object":

    print(f'{col.name}: {col.nunique()}')
    print(col.value_counts())

df.apply(unique_vals)

"""#### On basis of this info, which encoding technique should we use ?

 - We will use binary encoding for features with 2 or less unique val.
 - For features < 6 unique vals we will use OneHot encoding
 - Rest of the categorical features will be Target encoded

"""

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# Create a label encoder object
le = LabelEncoder()

def label_encode(ser):

    if ser.dtype=="object" and ser.nunique() <= 2:
      print(ser.name)

      le.fit(ser)
      ser = le.transform(ser)

    return ser

df = df.apply(lambda col: label_encode(col))
# convert rest of categorical variable into dummy
df = pd.get_dummies(df, columns = ["BusinessTravel", "Department", "MaritalStatus"], drop_first = True)
df.head()

df['WorkLifeBalance'].unique()

df['MaritalStatus_Married'] = df['MaritalStatus_Married'].map({True:1, False:0})

df_new = df[['MaritalStatus_Married','StockOptionLevel','JobSatisfaction','MonthlyIncome','MonthlyRate','OverTime','Age','Attrition',
             'EducationField','WorkLifeBalance', 'JobRole']]

df

target = df_new['Attrition'].copy()
df_new = df_new.drop(["Attrition"], axis = 1)
target.value_counts()

"""The dataset is extremely imbalanced
Recall how we deal with imbalanced data
For this dataset we will use SMOTE oversampling technique to balance the data

But SMOTE is applied only to training set

So we need to split the data first
"""

# Since we have class imbalance (i.e. more employees with turnover=0 than turnover=1)
# let's use stratify=y to maintain the same ratio as in the training dataset when splitting the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_new,
                                                    target,
                                                    test_size=0.25,
                                                    random_state=7,
                                                    stratify=target)

print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

!pip install category_encoders

X_train



import category_encoders as ce

ce_target = ce.TargetEncoder(cols = ['EducationField', 'JobRole'])
X_train = ce_target.fit_transform(X_train, y_train)
X_test = ce_target.transform(X_test)
import pickle
with open('ce_target4.pkl', 'wb') as f:
    pickle.dump(ce_target, f)

from google.colab import files
files.download('ce_target4.pkl')

X_train

from imblearn.over_sampling import SMOTE
from collections import Counter
sm = SMOTE()

X_sm, y_sm = sm.fit_resample(X_train, y_train)
print('Resampled dataset shape {}'.format(Counter(y_sm)))

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=7, max_depth=4, n_estimators=100)

X_sm

rfc = RandomForestClassifier(random_state=7)
param_grid = {
    'n_estimators': [100,200],
    'max_depth': [8, 10],
    'min_samples_split': [5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False],
    'criterion': ['gini', 'entropy'],
    'oob_score': [True],  # only when bootstrap=True
    'ccp_alpha': [0.02, 0.01]     # minimal cost-complexity pruning
}

# Setup GridSearchCV
grid = GridSearchCV(estimator=rfc,
                           param_grid=param_grid,
                           cv=4,
                           scoring='accuracy',  # you can change to 'f1', 'recall', etc.
                           n_jobs=-1,
                           verbose=2)

grid = grid.fit(X_sm, y_sm)
pred = grid.predict(X_test)

grid.best_estimator_

import pickle

with open('rf.pkl', 'wb') as f:
    pickle.dump(grid.best_estimator_, f)
from google.colab import files
files.download('rf.pkl')

# Confusion Matrix

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cnf_matrix = confusion_matrix(y_test, pred)
fig, ax = plt.subplots()

# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')

plt.tight_layout()
plt.title('Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

from sklearn.metrics import classification_report

print(classification_report(y_test, pred))

from google.colab import files
files.download("Employee_attrition.ipynb")

importances = grid.best_estimator_.feature_importances_
indices = np.argsort(importances)[::-1] # Sort feature importances in descending order
names = [X_train.columns[i] for i in indices] # Rearrange feature names so they match the sorted feature importances

plt.figure(figsize=(15, 7)) # Create plot
plt.title("Feature Importance") # Create plot title
plt.bar(range(X_sm.shape[1]), importances[indices]) # Add bars
plt.xticks(range(X_sm.shape[1]), names, rotation=90) # Add feature names as x-axis labels
plt.show() # Show plot

"""Means out of our actual number of employees who are gonna leave, we predicted 53%. This is a good number."""